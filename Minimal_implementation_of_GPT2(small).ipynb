{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNk38qUvwHk/5/nVCU0ZrI+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doudi25/GPT2-custom-smallest-one-/blob/main/Minimal_implementation_of_GPT2(small).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "vulIqHqANfeY"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Load the model\n",
        "model_original = GPT2LMHeadModel.from_pretrained('gpt2')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "input_text = \"can you tell me if you are a language model?\"\n",
        "\n",
        "# Tokenize the input text\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')  # Return PyTorch tensors\n"
      ],
      "metadata": {
        "id": "WRwgBapyN-KM"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmYFXPN3PmvT",
        "outputId": "fdf1e748-0c5a-4b56-f536-10912333e239"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[5171,  345, 1560,  502,  611,  345,  389,  257, 3303, 2746,   30]])"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "output = model_original.generate(input_ids, max_length=20, num_return_sequences=1)\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taAqsHeBPqTo",
        "outputId": "ce3c9d83-b37a-4d09-b9d4-4be2f3320d45"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "can you tell me if you are a language model?\n",
            "\n",
            "I'm not a language model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import tiktoken\n",
        "import time\n",
        "import inspect\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\"bias\",torch.tril(torch.ones(config.block_size,config.block_size)).view(1,1,config.block_size,config.block_size))\n",
        "    def forward(self,x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        query , key , value = qkv.split(self.n_embd,dim=2)\n",
        "\n",
        "        query = query.view(B,T,self.n_head,C // self.n_head).transpose(1,2)\n",
        "\n",
        "        key = key.view(B,T,self.n_head,C // self.n_head).transpose(1,2)\n",
        "\n",
        "        value = value.view(B,T,self.n_head,C // self.n_head).transpose(1,2)\n",
        "\n",
        "        attention_score = (query @ key.transpose(-2,-1)) * (1.0/ math.sqrt(key.size(-1)))\n",
        "        attention_score  = attention_score.masked_fill(self.bias[:,:,:T,:T]==0,float('-inf'))\n",
        "\n",
        "        attention_score = F.softmax(attention_score,dim = -1)\n",
        "\n",
        "        y = torch.matmul(attention_score,value)\n",
        "\n",
        "        y = y.transpose(1,2).contiguous().view(B,T,C)\n",
        "        y = self.c_proj(y)\n",
        "        return\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "    def forward(self,x):\n",
        "      x = self.c_fc(x)\n",
        "      x = F.gelu(x)\n",
        "      x = self.c_proj(x)\n",
        "      return x\n",
        "class Block(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "    def forward(self,x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.wte = nn.Embedding(config.vocab_size,config.n_embd)\n",
        "        self.wpe = nn.Embedding(config.block_size,config.n_embd)\n",
        "        self.layers = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size,bias=False)\n",
        "        self.wte.weight = self.lm_head.weight\n",
        "    def forward(self,idx,targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size , f\"Cannot forward sequence of length {T}\"\n",
        "        pos = torch.arange(0,T, dtype=torch.long,device= idx.device)\n",
        "        pos_emb = self.wpe(pos)\n",
        "        tok_emb = self.wte(idx)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.layers:\n",
        "            x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "o1Spep4fQ0Gf"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPTConfig()"
      ],
      "metadata": {
        "id": "5E6_7icxmm4T"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(config)"
      ],
      "metadata": {
        "id": "AihWWXm0mwzC"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z2CTkMRyyR3s",
        "outputId": "cb365981-83fb-43fe-d99c-a65a0ee8ec13"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save = model.state_dict()\n",
        "save2 = model_original.state_dict()"
      ],
      "metadata": {
        "id": "OOjog3T-ndj5"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys = save.keys()\n",
        "keys_original = save2.keys()"
      ],
      "metadata": {
        "id": "QfgmExTrni2v"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(keys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_coF7Nzlnnq-",
        "outputId": "9ee6eb9e-cf06-45ed-e4fb-28565042ff78"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "161"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(keys_original)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkS5oJq9nwQq",
        "outputId": "f7a3aaf1-0a32-496a-821a-53720fbba965"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sd_keys_hf = [k for k in keys_original if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "keys = [k for k in keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "assert len(sd_keys_hf) == len(keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(keys)}\"\n",
        "for k in zip(keys,sd_keys_hf):\n",
        "      if any(k[1].endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "          assert save2[k[1]].shape[::-1] == save[k[0]].shape\n",
        "          with torch.no_grad():\n",
        "                save[k[0]].copy_(save2[k[1]].t())\n",
        "      else:\n",
        "                # vanilla copy over the other parameters\n",
        "          assert save2[k[1]].shape == save[k[0]].shape\n",
        "          with torch.no_grad():\n",
        "                save[k[0]].copy_(save2[k[1]])\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aYc5BBS13wLQ"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# naive way of next token prediciton\n",
        "tokens = input_ids\n",
        "for i in range(30):  # Generate 30 new tokens\n",
        "    out = model(tokens)  # Forward pass through the model\n",
        "    out = out[:, -1, :]  # Get the logits for the last token\n",
        "    out = torch.softmax(out/0.4, dim=-1)  # Convert logits to probabilities\n",
        "    out = torch.multinomial(out, 1)  # Sample a token from the distribution\n",
        "    tokens = torch.cat([tokens, out], dim=1)  # Append the new token to the sequence and repeat the process"
      ],
      "metadata": {
        "id": "dL40gphWKlvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = tokenizer.decode(tokens[0],add_special_tokens=True) #decoding the tokens into string\n",
        "generated_text"
      ],
      "metadata": {
        "id": "7hkXqG9XPAw7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}